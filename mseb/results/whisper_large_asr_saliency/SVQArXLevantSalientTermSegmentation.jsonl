{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term", "task_metadata": {"name": "SVQArXLevantSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ar_x_levant.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ar-x-levant"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 12896.0, "min": 0.0, "max": 37735.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 16294.0, "min": 0.0, "max": 37735.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 22274.0, "min": 0.0, "max": 37735.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 37735.0, "min": 0.0, "max": 37735.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.3417516894130118, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.4318007155160991, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.5902742811713264, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.40815629881942567, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.5354975486948457, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.28654366988595126, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 78.30698412698413, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:clean", "task_metadata": {"name": "SVQArXLevantSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ar_x_levant.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ar-x-levant"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 3293.0, "min": 0.0, "max": 9472.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 4081.0, "min": 0.0, "max": 9472.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 5877.0, "min": 0.0, "max": 9472.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 9472.0, "min": 0.0, "max": 9472.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.34765625, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.43084881756756754, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.6204603040540541, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.440838126828965, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.49989442567567566, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.300029357476296, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 78.30698412698413, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:media_noise", "task_metadata": {"name": "SVQArXLevantSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ar_x_levant.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ar-x-levant"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 3522.0, "min": 0.0, "max": 10252.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 4388.0, "min": 0.0, "max": 10252.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 6254.0, "min": 0.0, "max": 10252.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 10252.0, "min": 0.0, "max": 10252.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.3435427233710496, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.4280140460397971, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.6100273117440499, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.42872227829822024, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.5150214592274678, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.29377344481772444, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 78.30698412698413, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:traffic_noise", "task_metadata": {"name": "SVQArXLevantSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ar_x_levant.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ar-x-levant"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 3025.0, "min": 0.0, "max": 8595.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3946.0, "min": 0.0, "max": 8595.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 4873.0, "min": 0.0, "max": 8595.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 8595.0, "min": 0.0, "max": 8595.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.35194880744618967, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.4591041303083188, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.56695753344968, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.3872271371125336, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.5598603839441536, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.2870145166835927, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 78.30698412698413, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:background_speech", "task_metadata": {"name": "SVQArXLevantSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ar_x_levant.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ar-x-levant"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 3056.0, "min": 0.0, "max": 9416.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3879.0, "min": 0.0, "max": 9416.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 5270.0, "min": 0.0, "max": 9416.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 9416.0, "min": 0.0, "max": 9416.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.324553950722175, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.41195836873406966, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.5596856414613424, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.3719911348081958, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.5713678844519966, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.2661766021589811, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 78.30698412698413, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
