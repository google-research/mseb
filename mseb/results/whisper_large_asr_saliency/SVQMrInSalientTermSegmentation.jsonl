{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term", "task_metadata": {"name": "SVQMrInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for mr_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["mr-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 1719.0, "min": 0.0, "max": 11089.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 2800.0, "min": 0.0, "max": 11089.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 2596.0, "min": 0.0, "max": 11089.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 11089.0, "min": 0.0, "max": 11089.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.15501848678870953, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.2525024799350708, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.23410587068265848, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.11005202106482515, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8691496077193616, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.11148708005323976, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 72.64017301973506, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:clean", "task_metadata": {"name": "SVQMrInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for mr_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["mr-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 475.0, "min": 0.0, "max": 2782.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 765.0, "min": 0.0, "max": 2782.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 708.0, "min": 0.0, "max": 2782.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2782.0, "min": 0.0, "max": 2782.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.17074047447879223, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.27498202731847593, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.2544931703810209, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.11689699305477298, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8594536304816679, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.12236672918312044, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 72.64017301973506, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:media_noise", "task_metadata": {"name": "SVQMrInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for mr_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["mr-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 450.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 708.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 730.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2770.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.1624548736462094, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.2555956678700361, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.26353790613718414, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.1320573413104261, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8483754512635379, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.11515270904969385, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 72.64017301973506, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:traffic_noise", "task_metadata": {"name": "SVQMrInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for mr_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["mr-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 349.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 599.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 556.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2770.0, "min": 0.0, "max": 2770.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.1259927797833935, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.21624548736462093, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.2007220216606498, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.09225732704742201, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8866425992779784, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.09218177296964361, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 72.64017301973506, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:background_speech", "task_metadata": {"name": "SVQMrInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for mr_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["mr-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 445.0, "min": 0.0, "max": 2767.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 728.0, "min": 0.0, "max": 2767.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 602.0, "min": 0.0, "max": 2767.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2767.0, "min": 0.0, "max": 2767.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.16082399710878206, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.2631008312251536, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.21756414889772316, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.09895478093316057, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8821828695337911, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.11694501189941442, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 72.64017301973506, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
