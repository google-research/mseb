{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term", "task_metadata": {"name": "SVQGuInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for gu_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["gu-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 2692.0, "min": 0.0, "max": 11067.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3771.0, "min": 0.0, "max": 11067.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3520.0, "min": 0.0, "max": 11067.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 11067.0, "min": 0.0, "max": 11067.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.24324568537092256, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.3407427487123882, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.3180627089545496, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.1592459061797821, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8116924189030451, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.22783073755178126, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 73.79343995662781, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:clean", "task_metadata": {"name": "SVQGuInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for gu_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["gu-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 726.0, "min": 0.0, "max": 2763.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 1003.0, "min": 0.0, "max": 2763.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 1002.0, "min": 0.0, "max": 2763.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2763.0, "min": 0.0, "max": 2763.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.26275787187839306, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.3630112196887441, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.36264929424538545, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.19358966548421913, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.7752442996742671, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.2251599520664278, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 73.79343995662781, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:media_noise", "task_metadata": {"name": "SVQGuInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for gu_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["gu-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 728.0, "min": 0.0, "max": 2802.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 1030.0, "min": 0.0, "max": 2802.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 881.0, "min": 0.0, "max": 2802.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2802.0, "min": 0.0, "max": 2802.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.2598144182726624, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.36759457530335476, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.31441827266238404, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.1521539124915443, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.815845824411135, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.25424054670155294, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 73.79343995662781, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:traffic_noise", "task_metadata": {"name": "SVQGuInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for gu_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["gu-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 575.0, "min": 0.0, "max": 2733.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 808.0, "min": 0.0, "max": 2733.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 822.0, "min": 0.0, "max": 2733.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2733.0, "min": 0.0, "max": 2733.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.21039151115989754, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.2956458104646908, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.300768386388584, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.1527022549584, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8225393340651299, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.2011631106841808, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 73.79343995662781, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:background_speech", "task_metadata": {"name": "SVQGuInSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for gu_in.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["gu-IN"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 663.0, "min": 0.0, "max": 2769.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 930.0, "min": 0.0, "max": 2769.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 815.0, "min": 0.0, "max": 2769.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 2769.0, "min": 0.0, "max": 2769.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.23943661971830985, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.3358613217768147, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.29433008306247743, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.13861165487762245, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.8331527627302275, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.23172627254247471, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 73.79343995662781, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
