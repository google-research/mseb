{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term", "task_metadata": {"name": "SVQKoKrSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ko_kr.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ko-KR"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 5744.0, "min": 0.0, "max": 18913.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 6031.0, "min": 0.0, "max": 18913.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 13677.0, "min": 0.0, "max": 18913.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 18913.0, "min": 0.0, "max": 18913.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.3037064453021731, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.31888119283032834, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.7231533865595093, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.5980759738418111, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.3904258132769109, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.27047763809256203, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 51.456441717791414, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:clean", "task_metadata": {"name": "SVQKoKrSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ko_kr.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ko-KR"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 1446.0, "min": 0.0, "max": 4705.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 1513.0, "min": 0.0, "max": 4705.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3434.0, "min": 0.0, "max": 4705.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4705.0, "min": 0.0, "max": 4705.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.3073326248671626, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.32157279489904356, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.7298618490967056, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.6149470325170423, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.3837975760153094, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.27233855740936164, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 51.456441717791414, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:media_noise", "task_metadata": {"name": "SVQKoKrSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ko_kr.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ko-KR"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 1459.0, "min": 0.0, "max": 4735.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 1511.0, "min": 0.0, "max": 4735.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3604.0, "min": 0.0, "max": 4735.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4735.0, "min": 0.0, "max": 4735.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.30813093980992606, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.3191129883843717, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.7611404435058078, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.6455213811679947, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.33995351785336997, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.28565149001424517, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 51.456441717791414, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:traffic_noise", "task_metadata": {"name": "SVQKoKrSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ko_kr.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ko-KR"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 1346.0, "min": 0.0, "max": 4725.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 1414.0, "min": 0.0, "max": 4725.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3220.0, "min": 0.0, "max": 4725.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4725.0, "min": 0.0, "max": 4725.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.2848677248677249, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.2992592592592593, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.6814814814814815, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.5490396183513707, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.44526783823840776, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.25491494976551077, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 51.456441717791414, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:background_speech", "task_metadata": {"name": "SVQKoKrSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for ko_kr.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["ko-KR"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 1493.0, "min": 0.0, "max": 4748.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 1593.0, "min": 0.0, "max": 4748.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3419.0, "min": 0.0, "max": 4748.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4748.0, "min": 0.0, "max": 4748.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.31444818871103625, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.3355096882898062, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.7200926705981466, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.5827967920480138, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.39275179098187946, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.27147781456283304, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 51.456441717791414, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
