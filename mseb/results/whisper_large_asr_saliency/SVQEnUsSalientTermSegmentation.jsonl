{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term", "task_metadata": {"name": "SVQEnUsSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for en_us.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["en-US"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 12333.0, "min": 0.0, "max": 16899.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 12818.0, "min": 0.0, "max": 16899.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 14670.0, "min": 0.0, "max": 16899.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 16899.0, "min": 0.0, "max": 16899.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.7298064974258832, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.7585064204982543, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.8680987040653293, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.809007361333486, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.1707793360553879, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.6458970902736096, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 96.63483046334103, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:clean", "task_metadata": {"name": "SVQEnUsSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for en_us.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["en-US"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 2967.0, "min": 0.0, "max": 4206.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3040.0, "min": 0.0, "max": 4206.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3846.0, "min": 0.0, "max": 4206.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4206.0, "min": 0.0, "max": 4206.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.7054208273894437, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.7227769852591536, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.9144079885877318, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.8672564822808722, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.11459819305753685, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.5969039230507166, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 96.63483046334103, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:media_noise", "task_metadata": {"name": "SVQEnUsSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for en_us.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["en-US"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 3269.0, "min": 0.0, "max": 4236.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3391.0, "min": 0.0, "max": 4236.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3677.0, "min": 0.0, "max": 4236.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4236.0, "min": 0.0, "max": 4236.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.7717186024551463, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.800519357884797, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.8680358829084042, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.8050506530959859, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.17398489140698772, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.7008524263372695, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 96.63483046334103, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:traffic_noise", "task_metadata": {"name": "SVQEnUsSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for en_us.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["en-US"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 3253.0, "min": 0.0, "max": 4224.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3379.0, "min": 0.0, "max": 4224.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3718.0, "min": 0.0, "max": 4224.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4224.0, "min": 0.0, "max": 4224.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.7701231060606061, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.7999526515151515, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.8802083333333334, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.8254267051829225, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.1534090909090909, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.6900719594110647, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 96.63483046334103, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
{"name": "whisper_large_asr_saliency", "sub_task_name": "salient_term:background_speech", "task_metadata": {"name": "SVQEnUsSalientTermSegmentation", "description": "Salient term segmentation task on the Simple Voice Questions (SVQ) dataset for en_us.", "reference": "TODO", "type": "SalientTermSegmentation", "category": "speech", "main_score": "NDCG", "revision": "1.0.0", "dataset": {"path": "https://huggingface.co/datasets/google/svq", "revision": "1.0.0", "documentation_file": null}, "scores": [{"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.0, "min": 0.0, "max": Infinity, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}], "eval_splits": ["test"], "eval_langs": ["en-US"], "domains": ["speech"], "task_subtypes": ["segmentation"], "documentation_file": null, "dataset_documentation_file": null}, "scores": [{"metric": "TimestampsAndEmbeddingsHits", "description": "The raw count of segments matching in both content and time.", "value": 2844.0, "min": 0.0, "max": 4233.0, "std": null}, {"metric": "TimestampsHits", "description": "The raw count of reference segments with a temporally aligned prediction.", "value": 3008.0, "min": 0.0, "max": 4233.0, "std": null}, {"metric": "EmbeddingsHits", "description": "The raw count of reference segments with a content-matched prediction.", "value": 3429.0, "min": 0.0, "max": 4233.0, "std": null}, {"metric": "NumSegments", "description": "The total number of ground-truth segments in the reference.", "value": 4233.0, "min": 0.0, "max": 4233.0, "std": null}, {"metric": "TimestampsAndEmbeddingsAccuracy", "description": "Overall Accuracy: The percentage of segments where the embedding is correct AND its associated start/end times are within the tolerance (tau). This is the strictest metric.", "value": 0.671863926293409, "min": 0.0, "max": 1.0, "std": null}, {"metric": "TimestampsAccuracy", "description": "Temporal Precision: The percentage of predicted segments whose start and end times are both within a specified tolerance (tau) of the reference timestamps.", "value": 0.7106071344200331, "min": 0.0, "max": 1.0, "std": null}, {"metric": "EmbeddingsAccuracy", "description": "Content Accuracy: The percentage of predicted segments where the embedding (e.g., a transcribed label) exactly matches the reference, irrespective of its timing.", "value": 0.8100637845499645, "min": 0.0, "max": 1.0, "std": null}, {"metric": "NDCG", "description": "Normalized Discounted Cumulative Gain. A metric that evaluates the quality of a sequence by rewarding correct terms found in the correct order.", "value": 0.7387048583732511, "min": 0.0, "max": 1.0, "std": null}, {"metric": "WordErrorRate", "description": "Word Error Rate (WER) between the predicted and reference sequences. Lower is better.", "value": 0.24072761634774392, "min": 0.0, "max": Infinity, "std": null}, {"metric": "InvalidResultRate", "description": "The percentage of examples that have an invalid segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "MissingResultRate", "description": "The percentage of examples that have no segmentation result. Lower is better.", "value": 0.0, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mAP", "description": "Mean Average Precision, a ranking metric that evaluates detection performance based on confidence scores.", "value": 0.5998429183941056, "min": 0.0, "max": 1.0, "std": null}, {"metric": "mean_encoding_size_bytes", "description": "Mean encoding size in bytes.", "value": 96.63483046334103, "min": 0, "max": Infinity, "std": null}, {"metric": "flops", "description": "Flops used to encode.", "value": 0.0, "min": 0, "max": Infinity, "std": null}], "url": null}
