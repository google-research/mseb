{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook uses streaming for all data loading. This is crucial because the full SVQ audio dataset is hundreds of gigabytes. Streaming allows you to analyze the metadata and text immediately without downloading the audio files."
      ],
      "metadata": {
        "id": "ngLObLS_ApVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Installation\n",
        "\n",
        "This section sets up the environment. It installs the Hugging Face datasets library for data loading, soundfile for robust audio decoding (bypassing the specific errors we saw earlier), and tqdm for progress bars. It also imports all necessary Python libraries globally so they are available for every subsequent section."
      ],
      "metadata": {
        "id": "lP76EebqAuhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Setup\n",
        "# @markdown Run this cell first. It installs necessary dependencies and imports libraries.\n",
        "\n",
        "# Install dependencies (Quiet mode to reduce log noise)\n",
        "!pip install datasets pandas tqdm matplotlib soundfile -q\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "import io\n",
        "import textwrap\n",
        "\n",
        "print(\"‚úÖ Environment ready! You can now run the sections below.\")"
      ],
      "metadata": {
        "id": "CLYsqFYUA1Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High-Level Dataset Statistics\n",
        "\n",
        "This section performs a statistical analysis of the audio subset. Because the dataset is massive, it streams the metadata (without downloading audio files) to calculate:\n",
        "\n",
        "* Examples per Locale: How many queries exist for each language/region (e.g., en_us, ar_eg).\n",
        "\n",
        "* Environments: The distribution of recording conditions (e.g., clean, car_noise).\n",
        "\n",
        "* Unique Speakers per Language: A complex metric that groups data by locale and counts how many distinct speaker_ids contributed to each."
      ],
      "metadata": {
        "id": "tFRk2XvPBSgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# High-Level Dataset Statistics\n",
        "# @markdown Scans the dataset metadata to calculate Locales, Environments, and Speaker diversity.\n",
        "\n",
        "# Load the audio subset in streaming mode\n",
        "audio_ds = load_dataset(\"google/svq\", \"audio\", split=\"test\", streaming=True)\n",
        "\n",
        "data = []\n",
        "scan_limit = 200000  # @param {type:\"integer\"}\n",
        "print(f\"üìä Scanning first {scan_limit} rows to generate statistics...\")\n",
        "\n",
        "for i, row in tqdm(enumerate(audio_ds), total=scan_limit):\n",
        "    if i >= scan_limit:\n",
        "        break\n",
        "\n",
        "    # Collect only necessary columns\n",
        "    data.append({\n",
        "        'locale': row['locale'],\n",
        "        'environment': row['environment'],\n",
        "        'speaker_id': row['speaker_id']\n",
        "    })\n",
        "\n",
        "# Convert to Pandas DataFrame for analysis\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET STATISTICS (Sampled)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Examples per Locale\n",
        "print(\"\\n--- üåç Examples per Locale ---\")\n",
        "print(df['locale'].value_counts())\n",
        "\n",
        "# 2. Environments\n",
        "print(\"\\n--- üèôÔ∏è Recording Environments ---\")\n",
        "print(df['environment'].value_counts())\n",
        "\n",
        "# 3. Unique Speakers per Language\n",
        "print(\"\\n--- üó£Ô∏è Unique Speakers per Locale ---\")\n",
        "speaker_stats = df.groupby('locale')['speaker_id'].nunique().sort_values(ascending=False)\n",
        "print(speaker_stats)"
      ],
      "metadata": {
        "id": "ry5Md2g2BrR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate \"Gold Standard\" ID List\n",
        "\n",
        "This is a critical utility section. The SVQ dataset is hierarchical, meaning not every audio file has a corresponding answer span. This script performs a cross-reference scan:\n",
        "\n",
        "1. It scans the strictest subset (span_reasoning) to find IDs that\n",
        "have granular answer spans.\n",
        "\n",
        "2. It verifies these IDs exist in the clean English audio subset.\n",
        "\n",
        "3. It prints a list of valid utt_ids that are guaranteed to work in Section 4 (Visualization) and Section 5 (Retrieval)."
      ],
      "metadata": {
        "id": "2aWupIqanxLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate \"Gold Standard\" ID List (en_us)\n",
        "# @markdown detailed: Finds `utt_id`s in `en_us` that possess **Audio + Document + Passage + Span** data.\n",
        "\n",
        "print(\"üöÄ Starting Cross-Reference Scan...\")\n",
        "\n",
        "# 1. Collect IDs from the Strictest Subset (Span Reasoning)\n",
        "print(\"\\n1Ô∏è‚É£ Scanning 'span_reasoning_in_lang' to find granular answers...\")\n",
        "reasoning_ds = load_dataset(\"google/svq\", \"span_reasoning_in_lang\", split=\"test\", streaming=True)\n",
        "\n",
        "valid_ids = set()\n",
        "scan_limit = 50000 # Scan deep into the dataset\n",
        "\n",
        "for i, row in tqdm(enumerate(reasoning_ds), total=scan_limit):\n",
        "    if i >= scan_limit: break\n",
        "\n",
        "    # Check for English locale\n",
        "    if row.get('locale') == 'en_us':\n",
        "        uid = row.get('utt_id') or row.get('id')\n",
        "        if uid:\n",
        "            valid_ids.add(uid)\n",
        "\n",
        "print(f\"   -> Found {len(valid_ids)} candidates with reasoning spans.\")\n",
        "\n",
        "# 2. Verify against Audio Subset\n",
        "print(\"\\n2Ô∏è‚É£ Verifying candidates against 'audio_en_us_clean'...\")\n",
        "audio_ds = load_dataset(\"google/svq\", \"audio_en_us_clean\", split=\"test\", streaming=True)\n",
        "\n",
        "final_gold_list = []\n",
        "\n",
        "for row in tqdm(audio_ds):\n",
        "    if row['utt_id'] in valid_ids:\n",
        "        final_gold_list.append(row['utt_id'])\n",
        "\n",
        "# 3. Output\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"‚úÖ FOUND {len(final_gold_list)} COMPLETE GOLD IDs\")\n",
        "print(\"=\"*50)\n",
        "print(\"Copy any ID below to use in the Visualization and Retrieval sections:\\n\")\n",
        "\n",
        "for uid in final_gold_list[:15]:\n",
        "    print(uid)\n",
        "\n",
        "print(f\"\\n(Total valid IDs found: {len(final_gold_list)})\")"
      ],
      "metadata": {
        "id": "irUAB2_qoCI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization & Inspection\n",
        "\n",
        "This section visualizes the audio data.\n",
        "\n",
        "* Parameters: Takes target_utt_id and target_locale.\n",
        "\n",
        "* Audio Processing: Uses a robust manual decoding method (soundfile + io.BytesIO) to avoid codec errors common in Colab.\n",
        "\n",
        "* Visualization: Plots the waveform and overlays colored highlights representing the Salient Terms (keywords) and their timestamps.\n",
        "\n",
        "* Playback: Provides an audio player to listen to the utterance."
      ],
      "metadata": {
        "id": "NcvPZqQBoq66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Waveform & Saliency Rank\n",
        "# @markdown Visualizes the audio and prints terms ordered by **Saliency (Importance)**.\n",
        "\n",
        "\n",
        "target_locale = \"en_us\" # @param [\"en_us\", \"ar_eg\", \"bn_in\", \"de_de\", \"es_es\", \"fa_ir\", \"fi_fi\", \"id_id\", \"ja_jp\", \"ko_kr\", \"ru_ru\", \"sw_ke\", \"te_in\", \"th_th\", \"tr_tr\", \"zh_cn\"]\n",
        "target_utt_id = \"utt_6232078546687499962\" # @param {type:\"string\"}\n",
        "\n",
        "def inspect_visuals(target_id, target_loc):\n",
        "    if not target_id:\n",
        "        print(\"‚ö†Ô∏è Please enter a valid utt_id.\")\n",
        "        return\n",
        "\n",
        "    print(f\"üîé Scanning audio subset for ID: {target_id}...\")\n",
        "\n",
        "    ds_stream = load_dataset(\"google/svq\", \"audio\", split=\"test\", streaming=True)\n",
        "\n",
        "    found_row = None\n",
        "    search_limit = 100000\n",
        "\n",
        "    for i, row in tqdm(enumerate(ds_stream), total=search_limit, desc=\"Scanning\"):\n",
        "        if i >= search_limit: break\n",
        "\n",
        "        if row['locale'] != target_loc:\n",
        "            continue\n",
        "\n",
        "        if row['utt_id'] == target_id:\n",
        "            found_row = row\n",
        "            break\n",
        "\n",
        "    if not found_row:\n",
        "        print(f\"\\n‚ùå ID '{target_id}' not found.\")\n",
        "        return\n",
        "\n",
        "    # --- Print Query ---\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üìù QUERY: \\\"{found_row['text']}\\\"\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # --- Manual Decode & Visualization ---\n",
        "    raw_audio = found_row['waveform']\n",
        "\n",
        "    if 'bytes' in raw_audio and raw_audio['bytes']:\n",
        "        audio_arr, sr = sf.read(io.BytesIO(raw_audio['bytes']))\n",
        "        duration = len(audio_arr) / sr\n",
        "        time_axis = np.linspace(0, duration, len(audio_arr))\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(16, 6))\n",
        "        plt.plot(time_axis, audio_arr, color='lightgray', label='Waveform')\n",
        "\n",
        "        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0']\n",
        "\n",
        "        terms = found_row['topk_salient_terms']\n",
        "        timestamps = found_row['topk_salient_terms_timestamps']\n",
        "\n",
        "        for i, (term, (start, end)) in enumerate(zip(terms, timestamps)):\n",
        "            color = colors[i % len(colors)]\n",
        "            plt.axvspan(start, end, color=color, alpha=0.5)\n",
        "\n",
        "            mid = start + (end - start)/2\n",
        "            plt.text(mid, max(audio_arr)*0.85, term,\n",
        "                     ha='center', fontsize=10, fontweight='bold',\n",
        "                     bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
        "\n",
        "        plt.title(f\"Utterance: {target_id}\", fontsize=14)\n",
        "        plt.xlabel(\"Time (s)\")\n",
        "        plt.ylabel(\"Amplitude\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nüîä Play Audio:\")\n",
        "        ipd.display(ipd.Audio(audio_arr, rate=sr))\n",
        "\n",
        "        # --- NEW: Print Saliency Rank ---\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚≠ê SALIENCY RANK (Most Important First)\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # The list in 'topk_salient_terms' is already ordered by importance score\n",
        "        for i, (term, (start, end)) in enumerate(zip(terms, timestamps), 1):\n",
        "            print(f\"Rank {i}: {term:<20} (Occurs at: {start:.2f}s - {end:.2f}s)\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Error: Audio bytes not accessible for this row.\")\n",
        "\n",
        "inspect_visuals(target_utt_id, target_locale)"
      ],
      "metadata": {
        "id": "7TB0QEjDMwSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Ground Truth Inspector\n",
        "\n",
        "This section retrieves the knowledge ground truth for a specific utterance.\n",
        "\n",
        "* Parameters: target_utt_id, target_locale, and retrieval_level.\n",
        "\n",
        "* Retrieval Levels: 1) Document: Finds the Wikipedia Page Title. 2) Passage: Finds the specific Paragraph (Context). 3) Span: Finds the exact short Answer string.\n",
        "\n",
        "* Formatting: Uses textwrap to ensure long passages are printed in a readable block format rather than a single long line."
      ],
      "metadata": {
        "id": "jEPp-p3SpAOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval Ground Truth Inspector\n",
        "# @markdown Select the **Retrieval Level** to see the Document, Context Passage, or Exact Answer Span.\n",
        "\n",
        "target_utt_id = \"utt_11477939086285057025\" # @param {type:\"string\"}\n",
        "target_locale = \"en_us\" # @param [\"en_us\", \"ar_eg\", \"bn_in\", \"de_de\", \"es_es\", \"fa_ir\", \"fi_fi\", \"id_id\", \"ja_jp\", \"ko_kr\", \"ru_ru\", \"sw_ke\", \"te_in\", \"th_th\", \"tr_tr\", \"zh_cn\"]\n",
        "retrieval_level = \"Span\" # @param [\"Document\", \"Passage\", \"Span\"]\n",
        "\n",
        "def find_retrieval_data(target_id, target_loc, level):\n",
        "    if not target_id:\n",
        "        print(\"‚ö†Ô∏è Please enter a valid utt_id.\")\n",
        "        return\n",
        "\n",
        "    # 1. Select Subset based on Level\n",
        "    if level == \"Document\":\n",
        "        subset = \"document_retrieval_in_lang\"\n",
        "    elif level == \"Passage\":\n",
        "        subset = \"span_retrieval_in_lang\"\n",
        "    else:\n",
        "        subset = \"span_reasoning_in_lang\"\n",
        "\n",
        "    print(f\"üîé Scanning subset '{subset}'...\")\n",
        "\n",
        "    # Load appropriate subset\n",
        "    ds = load_dataset(\"google/svq\", subset, split=\"test\", streaming=True)\n",
        "\n",
        "    found_data = None\n",
        "    scan_limit = 200000\n",
        "    match_count = 0\n",
        "\n",
        "    # 2. Scan and Filter\n",
        "    for i, row in tqdm(enumerate(ds), total=scan_limit, desc=f\"Scanning\"):\n",
        "        if i >= scan_limit: break\n",
        "\n",
        "        # Filter by locale\n",
        "        row_loc = row.get('locale', '')\n",
        "        if target_loc not in row_loc:\n",
        "            continue\n",
        "\n",
        "        match_count += 1\n",
        "\n",
        "        # Check ID (Robust check)\n",
        "        row_id = row.get('utt_id') or row.get('id')\n",
        "        if row_id == target_id:\n",
        "            found_data = row\n",
        "            break\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(f\"\\nüìä Scanned {i} rows. Found {match_count} candidates in locale '{target_loc}'.\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"üìö {level.upper()} GROUND TRUTH\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if found_data:\n",
        "        print(f\"‚úÖ EXACT MATCH FOUND!\")\n",
        "\n",
        "        q_text = found_data.get('question') or found_data.get('text')\n",
        "        title = found_data.get('document_title') or found_data.get('page_title') or found_data.get('title')\n",
        "\n",
        "        print(f\"üìù Query:        {q_text}\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"üìÑ DOCUMENT:     {title}\")\n",
        "\n",
        "        # --- Formatted Passage ---\n",
        "        if level in [\"Passage\", \"Span\"]:\n",
        "            context = found_data.get('context') or found_data.get('passage_text') or \"\"\n",
        "\n",
        "            # Wrap text to 80 chars\n",
        "            wrapper = textwrap.TextWrapper(width=80, initial_indent=\"  \", subsequent_indent=\"  \")\n",
        "            formatted_context = wrapper.fill(context)\n",
        "\n",
        "            print(\"-\" * 60)\n",
        "            print(\"üìñ PASSAGE:\")\n",
        "            print(formatted_context)\n",
        "\n",
        "        # --- Answer Span ---\n",
        "        if level == \"Span\":\n",
        "            answers = found_data.get('answers', 'N/A')\n",
        "            if isinstance(answers, dict):\n",
        "                ans_str = answers.get('text', [''])[0]\n",
        "            elif isinstance(answers, list):\n",
        "                ans_str = answers[0]\n",
        "            else:\n",
        "                ans_str = str(answers)\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"üéØ ANSWER SPAN:  \\\"{ans_str}\\\"\")\n",
        "\n",
        "        # --- Wikipedia Link ---\n",
        "        if title:\n",
        "            lang = target_loc.split('_')[0]\n",
        "            url = f\"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
        "            print(\"-\" * 60)\n",
        "            print(f\"üîó Wikipedia URL: {url}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ùå ID {target_id} not found in '{subset}'.\")\n",
        "        if level == \"Span\":\n",
        "             print(\"Tip: This ID might exist in 'Document' or 'Passage' level but lack a specific Span.\")\n",
        "\n",
        "find_retrieval_data(target_utt_id, target_locale, retrieval_level)"
      ],
      "metadata": {
        "id": "WH6YKRRRpvqr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}