{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [
        {
          "file_id": "1ejgb2T0p32jsrf4jpfzD-gHJKmLRjMml",
          "timestamp": 1765036216401
        }
      ],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Before starting:\n",
        "\n",
        "*  Go to Runtime menu and change runtime type to T4 GPU\n",
        "*  The first pip install cell will require restart of runtime after running.\n",
        "*  You must have HF_TOKEN colab secret set to download models: Use the key icon in the left side toolbar."
      ],
      "metadata": {
        "id": "D4IfyxTUqVur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xSdHELzGbJ0a"
      },
      "outputs": [],
      "source": [
        "# You will be asked to restart the session after this install.\n",
        "%%capture\n",
        "!pip install git+https://github.com/google-research/mseb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install scann\n",
        "!pip install openai-whisper"
      ],
      "metadata": {
        "id": "iF6nEqdNh_tH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mseb\n",
        "from mseb.datasets import parquet as parquet_datasets\n",
        "try:\n",
        "  # scann import throws because __init__ tries to enable unused tf ops.\n",
        "  from mseb.encoders import hf_llm_encoder\n",
        "except:\n",
        "  # but the deps are actually loaded, will succeed second try.\n",
        "  from mseb.encoders import hf_llm_encoder\n",
        "\n",
        "from pprint import pprint\n",
        "from IPython.display import Audio\n",
        "import json\n",
        "\n",
        "from absl import flags\n",
        "FLAGS = flags.FLAGS\n",
        "FLAGS([\"colab\",\n",
        "       \"--dataset_basepath=https://storage.googleapis.com/mseb_asru_tutorial\",\n",
        "       \"--task_cache_basepath=/tmp/cache\"])\n",
        "assert FLAGS.dataset_basepath"
      ],
      "metadata": {
        "id": "GUhShaVIbNMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set huggingface read token.\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['HF_TOKEN'] = userdata.get(\"HF_TOKEN\")"
      ],
      "metadata": {
        "id": "IC4SlbF-lVQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "import torch\n",
        "assert torch.cuda.device_count() > 0\n",
        "import gc"
      ],
      "metadata": {
        "id": "Q-9E5gQlp66o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "\n",
        "MSEB has six classes of task:\n",
        "*  classification\n",
        "*  clustering\n",
        "*  reasoning\n",
        "*  reranking\n",
        "*  retrieval\n",
        "*  segmentation\n",
        "\n",
        "For demonstration purposes, we have sampled small subsets of the evaluation datasets to use in colab and made them available on gcs. We'll override the datasets of a few tasks here to point to the pre-sampled demo data.\n",
        "\n",
        "We will look at two instances of tasks for this demo:\n",
        "*  **Intent Classification** (SpeechMassive)\n",
        "*  **Passage Retrieval** (SimpleVoiceQuestions)"
      ],
      "metadata": {
        "id": "qjYB8D_JyWlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intent classification (SpeechMassive).\n",
        "from mseb.tasks.classifications.intent import speech_massive\n",
        "\n",
        "# Override with the demo data.\n",
        "class SpeechMassiveFrFrIntentClassification(speech_massive.SpeechMassiveFrFrIntentClassification):\n",
        "  def _get_dataset(self):\n",
        "    return parquet_datasets.ParquetDataset(\n",
        "        dataset_name=\"speech_massive\",\n",
        "        task_name=\"SpeechMassiveFrFrIntentClassification\",\n",
        "        filename=\"SpeechMassiveDataset_task_language_fr-FR.parquet\",\n",
        "        sample_n=2)\n",
        "\n",
        "pprint(SpeechMassiveFrFrIntentClassification.metadata)"
      ],
      "metadata": {
        "id": "LVvpfJwlsJMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect sounds used by the task which will be passed through the encoder.\n",
        "task = SpeechMassiveFrFrIntentClassification()\n",
        "sound = next(task.sounds())\n",
        "pprint(sound)\n",
        "Audio(sound.waveform, rate=sound.context.sample_rate)"
      ],
      "metadata": {
        "id": "V_Zv80cI3yMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation examples.\n",
        "example = next(task.examples('passage_retrieval_in_lang'))\n",
        "pprint(example)"
      ],
      "metadata": {
        "id": "4FUiTpROE5dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Passage retrieval task (SimpleVoiceQuestions).\n",
        "#\n",
        "# We've hacked up a colab version of the task that avoids using scann for\n",
        "# index building and uses a (very) small cached dataset.\n",
        "#\n",
        "# You probably don't need to get into this implementation.\n",
        "from mseb.tasks.retrievals.passage_in_lang import svq as passage_retrieval_svq\n",
        "from mseb.datasets.parquet import ParquetDataset\n",
        "from mseb.evaluators import retrieval_evaluator\n",
        "from mseb import types as mseb_types\n",
        "\n",
        "class SVQEnUsPassageInLangRetrieval(passage_retrieval_svq.SVQEnUsPassageInLangRetrieval):\n",
        "\n",
        "  def __init__(self, transcript_key=\"whisper_transcript\",\n",
        "               context_key=\"top100_passages_gemini_embedding_whisper_transcript\"):\n",
        "    super().__init__()\n",
        "    self._transcript_key = transcript_key\n",
        "    self._context_key = context_key\n",
        "    self._ds = None\n",
        "    self._get_dataset()\n",
        "\n",
        "  def _get_dataset(self):\n",
        "    if not self._ds:\n",
        "      self._ds = ParquetDataset(\n",
        "          dataset_name=\"svq\",\n",
        "          task_name=\"passage_retrieval_in_lang\",\n",
        "          filename=\"SimpleVoiceQuestionsDataset_passage_retrieval_in_lang_locale_en_us.parquet\",\n",
        "          id_key=\"utt_id\",\n",
        "          sample_n=2)\n",
        "    return self._ds\n",
        "\n",
        "  def sounds(self):\n",
        "    svq_dataset = self._get_dataset()\n",
        "    for example in svq_dataset.get_task_data(\n",
        "        \"passage_retrieval_in_lang\",\n",
        "        dtype={\n",
        "            'locale': str,\n",
        "            'utt_id': str,\n",
        "            self._transcript_key: str,\n",
        "        },\n",
        "    ).itertuples():\n",
        "      if example.locale == self.locale:\n",
        "        sound = svq_dataset.get_sound(example._asdict())\n",
        "        sound.context.text = getattr(example, self._transcript_key)\n",
        "        if self._context_key:\n",
        "          sound = mseb_types.SoundWithTitleAndContext(\n",
        "            waveform=sound.waveform,\n",
        "            context_text=getattr(example, self._context_key),\n",
        "            context=sound.context,\n",
        "          )\n",
        "        yield sound\n",
        "\n",
        "  def documents(self):\n",
        "    ds = self._get_dataset().get_task_data(\"passage_retrieval_in_lang\")\n",
        "    top_passages = ds[self._context_key]\n",
        "    for x in top_passages:\n",
        "      for x in [json.loads(line) for line in x.splitlines() if line.strip()]:\n",
        "        yield mseb_types.Text(\n",
        "            text=x[\"text\"],\n",
        "            context=mseb_types.TextContextParams(id=x[\"id\"]))\n",
        "\n",
        "  def setup(\n",
        "      self, runner=None\n",
        "  ):\n",
        "    self._evaluator = retrieval_evaluator.RetrievalEvaluator(\n",
        "        searcher=None, id_by_index_id={}\n",
        "    )\n",
        "\n",
        "pprint(SVQEnUsPassageInLangRetrieval.metadata)"
      ],
      "metadata": {
        "id": "xLTvnPoM-s0a",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sound.\n",
        "task = SVQEnUsPassageInLangRetrieval()\n",
        "sound = next(task.sounds())\n",
        "pprint(sound)\n",
        "Audio(sound.waveform, rate=sound.context.sample_rate)"
      ],
      "metadata": {
        "id": "qKahVLQqABSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The examples evaluated by the class.\n",
        "example = next(task.examples('passage_retrieval_in_lang'))\n",
        "pprint(example)"
      ],
      "metadata": {
        "id": "HVBbV8ur5Gpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passage corpus.\n",
        "corpus = task.documents()\n",
        "pprint(next(corpus))"
      ],
      "metadata": {
        "id": "VAc8MXSMh8Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoders"
      ],
      "metadata": {
        "id": "ELQ9yt5nn2DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A very simple Text -> TextEmbedding encoder implementation.\n",
        "from mseb import encoder as encoder_lib\n",
        "from mseb import types as mseb_types\n",
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "class Rot13Encoder(encoder_lib.MultiModalEncoder):\n",
        "\n",
        "  def _setup(self):\n",
        "    self._codec = 'rot13'\n",
        "\n",
        "  def _check_input_types(self, batch):\n",
        "    if not all(isinstance(x, mseb_types.Text) for x in batch):\n",
        "      raise ValueError('Batch must be all Text input.')\n",
        "\n",
        "  def _encode(self, batch):\n",
        "    return [\n",
        "        mseb_types.TextEmbedding(\n",
        "            embedding=np.array(codecs.encode(x.text)),\n",
        "            spans=np.array([[0, len(x.text)]]),\n",
        "            context=x.context)\n",
        "        for x in batch\n",
        "    ]\n",
        "\n",
        "rot13_encoder = Rot13Encoder()\n",
        "text = mseb_types.Text(\n",
        "    text=\"hello world\",\n",
        "    context=mseb_types.TextContextParams(id='test000'))\n",
        "pprint(rot13_encoder.encode([text]))"
      ],
      "metadata": {
        "id": "P_4p-YWT0Sd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from mseb.encoders import whisper_encoder"
      ],
      "metadata": {
        "id": "oMty451CzmDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An embedding could be Sound -> SoundEmbedding(text) (ASR)\n",
        "whisper_asr_medium = whisper_encoder.SpeechToTextEncoder(\"medium\")\n",
        "whisper_asr_medium.setup()\n",
        "\n",
        "task = SVQEnUsPassageInLangRetrieval()\n",
        "sound = next(task.sounds())\n",
        "pprint(sound)\n",
        "encoded, = whisper_asr_medium.encode([sound])\n",
        "pprint(encoded)\n",
        "Audio(sound.waveform, rate=sound.context.sample_rate)\n",
        "del whisper_asr_medium\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "CQVG3ThAsA9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An embedding could be Sound -> SoundEmbedding(fixed size vector) (ASR)\n",
        "whisper_pooled_medium = whisper_encoder.PooledAudioEncoder(\"medium\")\n",
        "whisper_pooled_medium.setup()\n",
        "\n",
        "task = SVQEnUsPassageInLangRetrieval()\n",
        "sound = next(task.sounds())\n",
        "pprint(sound)\n",
        "encoded, = whisper_pooled_medium.encode([sound])\n",
        "pprint(encoded)\n",
        "Audio(sound.waveform, rate=sound.context.sample_rate)\n",
        "del whisper_pooled_medium\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "5Dk9S5xu0ikv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mseb.encoders import gecko_encoder\n",
        "from mseb.encoders import prompt_registry\n",
        "\n",
        "gemma_intent_classification = hf_llm_encoder.HFLLMWithTitleAndContextEncoder(\n",
        "  model_path=\"google/gemma-3n-E2B-it\",\n",
        "  prompt=prompt_registry.get_prompt_metadata(\"intent_classification\").load(),\n",
        ")"
      ],
      "metadata": {
        "id": "IRVY4VKpxK-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt is part of the encoding process for a particular use case.\n",
        "prompt = prompt_registry.get_prompt_metadata(\"intent_classification\").load()\n",
        "pprint(prompt.GetPromptTemplate())"
      ],
      "metadata": {
        "id": "4AL2jI566c4R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run encoder: gemma3n prompted to produce the class labels for intent classification.\n",
        "gemma_intent_classification.setup()\n",
        "encoded = gemma_intent_classification.encode([sound])\n",
        "pprint(encoded)"
      ],
      "metadata": {
        "id": "1dNM8ZaA5_I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running task/encoder benchmarks.\n",
        "\n",
        "The mechanism for running the encoder across as task is help in the Runner.\n",
        "For this colab, the runner will be the DirectRunner that simply iterates over the data in python and executes the encoder locally. A BeamRunner is supplied\n",
        "for running in distributed settings, and the Runner interface can be implemented\n",
        "by an end-user to customize how to distribute work in their local environement."
      ],
      "metadata": {
        "id": "IQdJQEVwzIZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mseb.runner import DirectRunner\n",
        "\n",
        "# The runner allows bulk encoding, it produces a mapping from instance ids to results.\n",
        "runner = DirectRunner(encoder=gemma_intent_classification)\n",
        "task = SpeechMassiveFrFrIntentClassification()\n",
        "results_cache = runner.run(task.sounds())\n",
        "print(\"\\nSound ids:\", results_cache.keys())\n",
        "key = list(results_cache.keys())[0]\n",
        "print(f\"Result[{key}]:\")\n",
        "pprint(results_cache[key])"
      ],
      "metadata": {
        "id": "tSAjqlI18CkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a task. This is how the run_task script runs a benchmark.\n",
        "\n",
        "from mseb.runner import DirectRunner\n",
        "from mseb.leaderboard import run_benchmark\n",
        "\n",
        "def run_task(encoder_name, encoder, task):\n",
        "  runner = DirectRunner(encoder=encoder)\n",
        "\n",
        "  # Setup runs global task pre-processing, for instance, running encoder over\n",
        "  # corpous for retrieval and building index. Not all tasks will have a setup\n",
        "  # step.\n",
        "  task.setup(runner=runner)\n",
        "\n",
        "  # Run benchmark uses the runner to encode all the task sounds and then calls\n",
        "  # the task evaluator.\n",
        "  return run_benchmark(encoder_name=encoder, runner=runner, task=task)\n",
        "\n",
        "result = run_task(\n",
        "    encoder_name=\"gemma_intent_classification\",\n",
        "    encoder=gemma_intent_classification,\n",
        "    task=SpeechMassiveFrFrIntentClassification())\n",
        "pprint(result)\n"
      ],
      "metadata": {
        "id": "krBexX_V-rjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cached retrieval encoder - for fast colab demo\n",
        "from typing import Callable, Sequence, final\n",
        "from mseb import encoder as encoder_lib\n",
        "from mseb.encoders import converter as converter_lib\n",
        "from mseb.encoders import prompt as prompt_lib\n",
        "from mseb.encoders import retrieval_encoder\n",
        "from mseb import types as mseb_types\n",
        "\n",
        "\n",
        "retrieval_encoder.RetrievalEncoder._setup = lambda self: _\n",
        "\n",
        "class CachedRetrievalEncoder(converter_lib.Converter):\n",
        "\n",
        "  def __init__(self, for_rag: bool, top_k:int = 10):\n",
        "    super().__init__()\n",
        "    self._for_rag = for_rag\n",
        "    self._top_k = top_k\n",
        "\n",
        "  @final\n",
        "  def _check_input_types(\n",
        "      self,\n",
        "      batch: Sequence[mseb_types.MultiModalObject] | Sequence[mseb_types.TextEmbedding],\n",
        "  ):\n",
        "    if not all(isinstance(x, mseb_types.SoundWithTitleAndContext) for x in batch):\n",
        "      raise ValueError(\n",
        "          'CachedRetrievalEncoder only supports a batch of all'\n",
        "          ' SoundWithTitleAndContext inputs.'\n",
        "      )\n",
        "\n",
        "  @final\n",
        "  def _encode(\n",
        "      self, batch: Sequence[mseb_types.MultiModalObject]\n",
        "  ) -> Sequence[mseb_types.TextPrediction]:\n",
        "    outputs = []\n",
        "    for sound in batch:\n",
        "      assert isinstance(sound, mseb_types.SoundWithTitleAndContext)\n",
        "      topk_retrieved_items = sound.context_text.split('\\n')[:self._top_k]\n",
        "      if self._for_rag:\n",
        "        output = mseb_types.TextWithTitleAndContext(\n",
        "          text='\\n'.join(topk_retrieved_items),\n",
        "          context=mseb_types.TextContextParams(\n",
        "              id=sound.context.id,\n",
        "              text=sound.context.text,\n",
        "          ),\n",
        "          context_text=sound.context_text,\n",
        "      )\n",
        "      else:\n",
        "        output = mseb_types.TextPrediction(\n",
        "                prediction='\\n'.join(topk_retrieved_items),\n",
        "                context=mseb_types.PredictionContextParams(\n",
        "                    id=sound.context.id, debug_text=sound.context.debug_text\n",
        "                ),\n",
        "            )\n",
        "      outputs.append(output)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def CachedRagHFLLMWithTitleAndContextTranscriptTruthEncoder(\n",
        "    model_path: str,\n",
        "    top_k: int = 10,\n",
        "    normalizer: Callable[[str], str] | None = None,\n",
        "    prompt: prompt_lib.Prompt = prompt_lib.RetrievalPrompt(),\n",
        ") -> encoder_lib.CascadeEncoder:\n",
        "  \"\"\"Cascaded transcript truth and RAG HF LLM encoder.\"\"\"\n",
        "  return encoder_lib.CascadeEncoder(\n",
        "      encoders=[\n",
        "          CachedRetrievalEncoder(for_rag=True),\n",
        "          hf_llm_encoder.HFLLMEncoder(\n",
        "              model_path=model_path, normalizer=normalizer, prompt=prompt\n",
        "          ),\n",
        "          converter_lib.TextEmbeddingToTextPredictionConverter(),\n",
        "      ]\n",
        "  )"
      ],
      "metadata": {
        "id": "Xqymo-pfdS_L",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_gemini_embedding_encoder = CachedRetrievalEncoder(for_rag=False)\n",
        "\n",
        "result = run_task(\n",
        "    encoder_name=\"retrieval_gemini_embedding_encoder\",\n",
        "    encoder=retrieval_gemini_embedding_encoder,\n",
        "    task=SVQEnUsPassageInLangRetrieval())\n",
        "pprint(result)"
      ],
      "metadata": {
        "id": "6Y0YJJ7Qfysq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
